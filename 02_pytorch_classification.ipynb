{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/02_pytorch_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/02_pytorch_classification.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/02_pytorch_classification.pdf) | [Watch Video Walkthrough](https://youtu.be/Z_ikDlimN6A?t=30691)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSLHJiQxH4jU"
   },
   "source": [
    "## 0. Architecture of a classification neural network\n",
    "\n",
    "Before we get into writing code, let's look at the general architecture of a classification neural network.\n",
    "\n",
    "| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n",
    "| --- | --- | --- |\n",
    "| **Input layer shape** (`in_features`) | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n",
    "| **Hidden layer(s)** | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n",
    "| **Neurons per hidden layer** | Problem specific, generally 10 to 512 | Same as binary classification |\n",
    "| **Output layer shape** (`out_features`) | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n",
    "| **Hidden layer activation** | Usually [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) (rectified linear unit) but [can be many others](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) | Same as binary classification |\n",
    "| **Output activation** | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) ([`torch.sigmoid`](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) in PyTorch)| [Softmax](https://en.wikipedia.org/wiki/Softmax_function) ([`torch.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) in PyTorch) |\n",
    "| **Loss function** | [Binary crossentropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) in PyTorch) | Cross entropy ([`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in PyTorch) |\n",
    "| **Optimizer** | [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent), [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) (see [`torch.optim`](https://pytorch.org/docs/stable/optim.html) for more options) | Same as binary classification |\n",
    "\n",
    "Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on.\n",
    "\n",
    "But it's more than enough to get started.\n",
    "\n",
    "We're going to get hands-on with this setup throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwvxFEjKHC71"
   },
   "source": [
    "## 1. Make classification data and get it ready\n",
    "\n",
    "Let's begin by making some data.\n",
    "\n",
    "We'll use the [`make_circles()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html) method from Scikit-Learn to generate two circles with different coloured dots. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RGeZvHsyHC72"
   },
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "\n",
    "# Make 1000 samples \n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03, # a little bit of noise to the dots\n",
    "                    random_state=42) # keep random state so we get the same values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FwwzJnQV2jv"
   },
   "source": [
    "Alright, now let's view the first 5 `X` and `y` values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAb8vcIhWEO8",
    "outputId": "b7316d88-7733-4981-9b4a-0a98c7cdd829"
   },
   "source": [
    "print(f\"First 5 X features:\\n{X[:5]}\")\n",
    "print(f\"\\nFirst 5 y labels:\\n{y[:5]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATakj2bVWBou"
   },
   "source": [
    "Looks like there's two `X` values per one `y` value. \n",
    "\n",
    "Let's keep following the data explorer's motto of *visualize, visualize, visualize* and put them into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "XAAqx_8sHC73",
    "outputId": "cd6ef4fe-cda3-48db-f2a5-9820660eab14"
   },
   "source": [
    "# Make DataFrame of circle data\n",
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X1\": X[:, 0],\n",
    "    \"X2\": X[:, 1],\n",
    "    \"label\": y\n",
    "})\n",
    "circles.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FK2T7GpYW2BE"
   },
   "source": [
    "It looks like each pair of `X` features (`X1` and `X2`) has a label (`y`) value of either 0 or 1.\n",
    "\n",
    "This tells us that our problem is **binary classification** since there's only two options (0 or 1).\n",
    "\n",
    "How many values of each class are there?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cV8sVR9tHC74",
    "outputId": "dee5739f-1695-4e71-bb0b-de270f08b621"
   },
   "source": [
    "# Check different labels\n",
    "circles.label.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytQ5rm9eXa65"
   },
   "source": [
    "500 each, nice and balanced.\n",
    "\n",
    "Let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ANkSmESCHC75",
    "outputId": "89b2f9ac-728c-481d-f4ba-6192a8334758"
   },
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=X[:, 0], \n",
    "            y=X[:, 1], \n",
    "            c=y, \n",
    "            cmap=plt.cm.RdYlBu);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0e1oR27HC76"
   },
   "source": [
    "Alrighty, looks like we've got a problem to solve.\n",
    "\n",
    "Let's find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).\n",
    "\n",
    "> **Note:** This dataset is often what's considered a **toy problem** (a problem that's used to try and test things out on) in machine learning. \n",
    "> \n",
    "> But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny6_J7F4HC76"
   },
   "source": [
    "### 1.1 Input and output shapes\n",
    "\n",
    "One of the most common errors in deep learning is shape errors.\n",
    "\n",
    "Mismatching the shapes of tensors and tensor operations will result in errors in your models.\n",
    "\n",
    "We're going to see plenty of these throughout the course.\n",
    "\n",
    "And there's no surefire way to make sure they won't happen, they will.\n",
    "\n",
    "What you can do instead is continually familiarize yourself with the shape of the data you're working with.\n",
    "\n",
    "I like referring to it as input and output shapes.\n",
    "\n",
    "Ask yourself:\n",
    "\n",
    "\"What shapes are my inputs and what shapes are my outputs?\"\n",
    "\n",
    "Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0h--vKdHC77",
    "outputId": "16e65bb5-83e1-49c7-af5e-90ecede4eeae"
   },
   "source": [
    "# Check the shapes of our features and labels\n",
    "X.shape, y.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW7zolShbfDg"
   },
   "source": [
    "Looks like we've got a match on the first dimension of each.\n",
    "\n",
    "There's 1000 `X` and 1000 `y`. \n",
    "\n",
    "But what's the second dimension on `X`?\n",
    "\n",
    "It often helps to view the values and shapes of a single sample (features and labels).\n",
    "\n",
    "Doing so will help you understand what input and output shapes you'd be expecting from your model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-L5zobVHC78",
    "outputId": "b3a96ca8-45f1-47d1-a98b-c2a7be79c0d5"
   },
   "source": [
    "# View the first example of features and labels\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
    "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn692XaRaifl"
   },
   "source": [
    "This tells us the second dimension for `X` means it has two features (vector) where as `y` has a single feature (scalar).\n",
    "\n",
    "We have two inputs for one output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLyDPN6ZR_ho"
   },
   "source": [
    "### 1.2 Turn data into tensors and create train and test splits\n",
    "\n",
    "We've investigated the input and output shapes of our data, now let's prepare it for being used with PyTorch and for modelling.\n",
    "\n",
    "Specifically, we'll need to:\n",
    "1. Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).\n",
    "2. Split our data into training and test sets (we'll train a model on the training set to learn the patterns between `X` and `y` and then evaluate those learned patterns on the test dataset)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2gcR31aHC78",
    "outputId": "cd4e10c1-c358-4b74-81e0-bab7610197cc"
   },
   "source": [
    "# Turn data into tensors\n",
    "# Otherwise this causes issues with computations later on\n",
    "import torch\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# View the first five samples\n",
    "X[:5], y[:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9XNJv8lfmRG"
   },
   "source": [
    "Now our data is in tensor format, let's split it into training and test sets.\n",
    "\n",
    "To do so, let's use the helpful function [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from Scikit-Learn.\n",
    "\n",
    "We'll use `test_size=0.2` (80% training, 20% testing) and because the split happens randomly across the data, let's use `random_state=42` so the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DW6PU82BHC79",
    "outputId": "a2d3f3df-701e-44ef-a506-fd31d5443e90"
   },
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-wYDWy9gSRc"
   },
   "source": [
    "Nice! Looks like we've now got 800 training samples and 200 testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCyQ93VTHC79"
   },
   "source": [
    "## 2. Building a model\n",
    "\n",
    "We've got some data ready, now it's time to build a model.\n",
    "\n",
    "We'll break it down into a few parts.\n",
    "\n",
    "1. Setting up device agnostic code (so our model can run on CPU or GPU if it's available).\n",
    "2. Constructing a model by subclassing `nn.Module`.\n",
    "3. Defining a loss function and optimizer.\n",
    "4. Creating a training loop (this'll be in the next section).\n",
    "\n",
    "The good news is we've been through all of the above steps before in notebook 01.\n",
    "\n",
    "Except now we'll be adjusting them so they work with a classification dataset.\n",
    "\n",
    "Let's start by importing PyTorch and `torch.nn` as well as setting up device agnostic code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "2itMTezPRnVR",
    "outputId": "507bf4a3-b5ae-4943-aa21-4eb181b0d741"
   },
   "source": [
    "# Standard PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Fil1v04nXr"
   },
   "source": [
    "Excellent, now `device` is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it's available.\n",
    "\n",
    "How about we create a model?\n",
    "\n",
    "We'll want a model capable of handling our `X` data as inputs and producing something in the shape of our `y` data as outputs.\n",
    "\n",
    "In other words, given `X` (features) we want our model to predict `y` (label).\n",
    "\n",
    "This setup where you have features and labels is referred to as **supervised learning**. Because your data is telling your model what the outputs should be given a certain input.\n",
    "\n",
    "To create such a model it'll need to handle the input and output shapes of `X` and `y`.\n",
    "\n",
    "Remember how I said input and output shapes are important? Here we'll see why.\n",
    "\n",
    "Let's create a model class that:\n",
    "1. Subclasses `nn.Module` (almost all PyTorch models are subclasses of `nn.Module`).\n",
    "2. Creates 2 `nn.Linear` layers in the constructor capable of handling the input and output shapes of `X` and `y`.\n",
    "3. Defines a `forward()` method containing the forward pass computation of the model.\n",
    "4. Instantiates the model class and sends it to the target `device`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_EsAE3VHC7-",
    "outputId": "d9c976ea-e23f-4993-c847-a15b0bf7b0b5"
   },
   "source": [
    "# 1. Construct a model class that subclasses nn.Module\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
    "    \n",
    "    # 3. Define a forward method containing the forward pass computation\n",
    "    def forward(self, x):\n",
    "        # Return the output of layer_2, a single feature, the same shape as y\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
    "\n",
    "# 4. Create an instance of the model and send it to target device\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtdkMniZ7KyK"
   },
   "source": [
    "What's going on here?\n",
    "\n",
    "We've seen a few of these steps before.\n",
    "\n",
    "The only major change is what's happening between `self.layer_1` and `self.layer_2`.\n",
    "\n",
    "`self.layer_1` takes 2 input features `in_features=2` and produces 5 output features `out_features=5`.\n",
    "\n",
    "This is known as having 5 **hidden units** or **neurons**.\n",
    "\n",
    "This layer turns the input data from having 2 features to 5 features.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "This allows the model to learn patterns from 5 numbers rather than just 2 numbers, *potentially* leading to better outputs.\n",
    "\n",
    "I say potentially because sometimes it doesn't work.\n",
    "\n",
    "The number of hidden units you can use in neural network layers is a **hyperparameter** (a value you can set yourself) and there's no set in stone value you have to use.\n",
    "\n",
    "Generally more is better but there's also such a thing as too much. The amount you choose will depend on your model type and dataset you're working with. \n",
    "\n",
    "Since our dataset is small and simple, we'll keep it small.\n",
    "\n",
    "The only rule with hidden units is that the next layer, in our case, `self.layer_2` has to take the same `in_features` as the previous layer `out_features`.\n",
    "\n",
    "That's why `self.layer_2` has `in_features=5`, it takes the `out_features=5` from `self.layer_1` and performs a linear computation on them, turning them into `out_features=1` (the same shape as `y`).\n",
    "\n",
    "![A visual example of what a classification neural network with linear activation looks like on the tensorflow playground](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-tensorflow-playground-linear-activation.png)\n",
    "*A visual example of what a similar classification neural network to the one we've just built looks like. Try creating one of your own on the [TensorFlow Playground website](https://playground.tensorflow.org/).*\n",
    "\n",
    "You can also do the same as above using [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "\n",
    "`nn.Sequential` performs a forward pass computation of the input data through the layers in the order they appear."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7I8GyLjHC7_",
    "outputId": "def65504-863a-4361-816e-909dbfa4d624"
   },
   "source": [
    "# Replicate CircleModelV0 with nn.Sequential\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiRHq9mxFIf-"
   },
   "source": [
    "Woah, that looks much simpler than subclassing `nn.Module`, why not just always use `nn.Sequential`?\n",
    "\n",
    "`nn.Sequential` is fantastic for straight-forward computations, however, as the namespace says, it *always* runs in sequential order.\n",
    "\n",
    "So if you'd like something else to happen (rather than just straight-forward sequential computation) you'll want to define your own custom `nn.Module` subclass.\n",
    "\n",
    "Now we've got a model, let's see what happens when we pass some data through it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt339-_CC8sz",
    "outputId": "a4014167-4181-434e-ab75-905094229b3a"
   },
   "source": [
    "# Make predictions with the model\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7v8TVnqGMZh"
   },
   "source": [
    "Hmm, it seems there are the same amount of predictions as there are test labels but the predictions don't look like they're in the same form or shape as the test labels.\n",
    "\n",
    "We've got a couple steps we can do to fix this, we'll see these later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aoQn39pHC7_"
   },
   "source": [
    "### 2.1 Setup loss function and optimizer\n",
    "\n",
    "We've setup a loss (also called a criterion or cost function) and optimizer before in [notebook 01](https://www.learnpytorch.io/01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch).\n",
    "\n",
    "But different problem types require different loss functions. \n",
    "\n",
    "For example, for a regression problem (predicting a number) you might use mean absolute error (MAE) loss.\n",
    "\n",
    "And for a binary classification problem (like ours), you'll often use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) as the loss function.\n",
    "\n",
    "However, the same optimizer function can often be used across different problem spaces.\n",
    "\n",
    "For example, the stochastic gradient descent optimizer (SGD, `torch.optim.SGD()`) can be used for a range of problems, and the same applies to the Adam optimizer (`torch.optim.Adam()`). \n",
    "\n",
    "| Loss function/Optimizer | Problem type | PyTorch Code |\n",
    "| ----- | ----- | ----- |\n",
    "| Stochastic Gradient Descent (SGD) optimizer | Classification, regression, many others. | [`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) |\n",
    "| Adam Optimizer | Classification, regression, many others. | [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) |\n",
    "| Binary cross entropy loss | Binary classification | [`torch.nn.BCELossWithLogits`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) or [`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) |\n",
    "| Cross entropy loss | Multi-class classification | [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) |\n",
    "| Mean absolute error (MAE) or L1 Loss | Regression | [`torch.nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) | \n",
    "| Mean squared error (MSE) or L2 Loss | Regression | [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) |  \n",
    "\n",
    "*Table of various loss functions and optimizers, there are more but these are some common ones you'll see.*\n",
    "\n",
    "Since we're working with a binary classification problem, let's use a binary cross entropy loss function.\n",
    "\n",
    "> **Note:** Recall a **loss function** is what measures how *wrong* your model predictions are, the higher the loss, the worse your model.\n",
    ">\n",
    "> Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.\n",
    "\n",
    "PyTorch has two binary cross entropy implementations:\n",
    "1. [`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).\n",
    "2. [`torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) - This is the same as above except it has a sigmoid layer ([`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)) built-in (we'll see what this means soon).\n",
    "\n",
    "Which one should you use? \n",
    "\n",
    "The [documentation for `torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) states that it's more numerically stable than using `torch.nn.BCELoss()` after a `nn.Sigmoid` layer. \n",
    "\n",
    "So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of `nn.Sigmoid` and `torch.nn.BCELoss()` but that is beyond the scope of this notebook.\n",
    "\n",
    "Knowing this, let's create a loss function and an optimizer. \n",
    "\n",
    "For the optimizer we'll use `torch.optim.SGD()` to optimize the model parameters with learning rate 0.1.\n",
    "\n",
    "> **Note:** There's a [discussion on the PyTorch forums about the use of `nn.BCELoss` vs. `nn.BCEWithLogitsLoss`](https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4). It can be confusing at first but as with many things, it becomes easier with practice."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DjpQsdOZHC7_"
   },
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), \n",
    "                            lr=0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKmi3fp9wYnV"
   },
   "source": [
    "Now let's also create an **evaluation metric**.\n",
    "\n",
    "An evaluation metric can be used to offer another perspective on how your model is going.\n",
    "\n",
    "If a loss function measures how *wrong* your model is, I like to think of evaluation metrics as measuring how *right* it is.\n",
    "\n",
    "Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.\n",
    "\n",
    "After all, when evaluating your models it's good to look at things from multiple points of view.\n",
    "\n",
    "There are several evaluation metrics that can be used for classification problems but let's start out with **accuracy**.\n",
    "\n",
    "Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.\n",
    "\n",
    "For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.\n",
    "\n",
    "Let's write a function to do so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RcaZQR0mHC7_"
   },
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuplloDPxviL"
   },
   "source": [
    "Excellent! We can now use this function whilst training our model to measure it's performance alongside the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UpJKZ8PHC8A"
   },
   "source": [
    "## 3. Train model\n",
    "\n",
    "Okay, now we've got a loss function and optimizer ready to go, let's train a model.\n",
    "\n",
    "Do you remember the steps in a PyTorch training loop?\n",
    "\n",
    "If not, here's a reminder.\n",
    "\n",
    "Steps in training:\n",
    "\n",
    "<details>\n",
    "    <summary>PyTorch training loop steps</summary>\n",
    "    <ol>\n",
    "        <li><b>Forward pass</b> - The model goes through all of the training data once, performing its\n",
    "            <code>forward()</code> function\n",
    "            calculations (<code>model(x_train)</code>).\n",
    "        </li>\n",
    "        <li><b>Calculate the loss</b> - The model's outputs (predictions) are compared to the ground truth and evaluated\n",
    "            to see how\n",
    "            wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li>\n",
    "        <li><b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they\n",
    "            can be\n",
    "            recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li>\n",
    "        <li><b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model\n",
    "            parameter to\n",
    "            be updated (each parameter\n",
    "            with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence \"backwards\"\n",
    "            (<code>loss.backward()</code>).</li>\n",
    "        <li><b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code>\n",
    "            with respect to the loss\n",
    "            gradients in order to improve them (<code>optimizer.step()</code>).</li>\n",
    "    </ol>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMeDqPpsnst6"
   },
   "source": [
    "### 3.1 Going from raw model outputs to predicted labels (logits -> prediction probabilities -> prediction labels)\n",
    "\n",
    "Before the training loop steps, let's see what comes out of our model during the forward pass (the forward pass is defined by the `forward()` method).\n",
    "\n",
    "To do so, let's pass the model some data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5stsbDl0zKl",
    "outputId": "829a0842-0a37-455a-b058-3e547200f836"
   },
   "source": [
    "# View the frist 5 outputs of the forward pass on the test data\n",
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1sNl8D8fWXm"
   },
   "source": [
    "Since our model hasn't been trained, these outputs are basically random.\n",
    "\n",
    "But *what* are they?\n",
    "\n",
    "They're the output of our `forward()` method.\n",
    "\n",
    "Which implements two layers of `nn.Linear()` which internally calls the following equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias}\n",
    "$$\n",
    "\n",
    "The *raw outputs* (unmodified) of this equation ($y$) and in turn, the raw outputs of our model are often referred to as [**logits**](https://datascience.stackexchange.com/a/31045).\n",
    "\n",
    "That's what our model is outputing above when it takes in the input data ($x$ in the equation or `X_test` in the code), logits.\n",
    "\n",
    "However, these numbers are hard to interpret.\n",
    "\n",
    "We'd like some numbers that are comparable to our truth labels.\n",
    "\n",
    "To get our model's raw outputs (logits) into such a form, we can use the [sigmoid activation function](https://pytorch.org/docs/stable/generated/torch.sigmoid.html).\n",
    "\n",
    "Let's try it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGC7UmBi0s6E",
    "outputId": "3564ee3b-e34f-40a3-a40a-b9c9b948da04"
   },
   "source": [
    "# Use sigmoid on model logits\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxmWz_GVjZoB"
   },
   "source": [
    "Okay, it seems like the outputs now have some kind of consistency (even though they're still random).\n",
    "\n",
    "They're now in the form of **prediction probabilities** (I usually refer to these as `y_pred_probs`), in other words, the values are now how much the model thinks the data point belongs to one class or another.\n",
    "\n",
    "In our case, since we're dealing with binary classification, our ideal outputs are 0 or 1.\n",
    "\n",
    "So these values can be viewed as a decision boundary.\n",
    "\n",
    "The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.\n",
    "\n",
    "More specificially:\n",
    "* If `y_pred_probs` >= 0.5, `y=1` (class 1)\n",
    "* If `y_pred_probs` < 0.5, `y=0` (class 0)\n",
    "\n",
    "To turn our prediction probabilities into prediction labels, we can round the outputs of the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naZxlTTwlMwX",
    "outputId": "6134e47d-bbb2-46c3-e2ec-b4f42d517e39"
   },
   "source": [
    "# Find the predicted labels (round the prediction probabilities)\n",
    "y_preds = torch.round(y_pred_probs)\n",
    "\n",
    "# In full\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "# Check for equality\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Get rid of extra dimension\n",
    "y_preds.squeeze()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cMsgFWWmPLU"
   },
   "source": [
    "Excellent! Now it looks like our model's predictions are in the same form as our truth labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaQ0CN4ZmU1W",
    "outputId": "6b1cd7b4-f1d0-49b5-a8c3-338cf75c6cb0"
   },
   "source": [
    "y_test[:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXqUulG3maPH"
   },
   "source": [
    "This means we'll be able to compare our model's predictions to the test labels to see how well it's performing. \n",
    "\n",
    "To recap, we converted our model's raw outputs (logits) to prediction probabilities using a sigmoid activation function.\n",
    "\n",
    "And then converted the prediction probabilities to prediction labels by rounding them.\n",
    "\n",
    "> **Note:** The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we'll be looking at using the [softmax activation function](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) (this will come later on).\n",
    ">\n",
    "> And the use of the sigmoid activation function is not required when passing our model's raw outputs to the `nn.BCEWithLogitsLoss` (the \"logits\" in logits loss is because it works on the model's raw logits output), this is because it has a sigmoid function built-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va7gg8yxn6Sg"
   },
   "source": [
    "### 3.2 Building a training and testing loop\n",
    "\n",
    "Alright, we've discussed how to take our raw model outputs and convert them to prediction labels, now let's build a training loop.\n",
    "\n",
    "Let's start by training for 100 epochs and outputing the model's progress every 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFABVGo2HC8A",
    "outputId": "e0341074-b603-41d5-a389-c401b4934d73",
    "ExecuteTime": {
     "end_time": "2026-01-25T18:56:16.954645700Z",
     "start_time": "2026-01-25T18:56:16.881941500Z"
    }
   },
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.69299, Accuracy: 50.88% | Test loss: 0.69489, Test acc: 47.00%\n",
      "Epoch: 10 | Loss: 0.69299, Accuracy: 51.75% | Test loss: 0.69488, Test acc: 47.00%\n",
      "Epoch: 20 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69486, Test acc: 46.50%\n",
      "Epoch: 30 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69485, Test acc: 46.50%\n",
      "Epoch: 40 | Loss: 0.69298, Accuracy: 50.88% | Test loss: 0.69483, Test acc: 46.00%\n",
      "Epoch: 50 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69482, Test acc: 46.00%\n",
      "Epoch: 60 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69481, Test acc: 46.00%\n",
      "Epoch: 70 | Loss: 0.69298, Accuracy: 51.25% | Test loss: 0.69480, Test acc: 46.00%\n",
      "Epoch: 80 | Loss: 0.69298, Accuracy: 51.38% | Test loss: 0.69479, Test acc: 46.00%\n",
      "Epoch: 90 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69478, Test acc: 46.00%\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_pytorch_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
